{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **EDA y Machine Learning para Estimaci√≥n de Obesidad**\n",
        "\n",
        "### **Descripci√≥n del Problema**\n",
        "\n",
        "El dataset \"Estimation of Obesity Levels Based On Eating Habits and Physical Condition\" contiene datos de individuos de M√©xico, Per√∫ y Colombia, con atributos relacionados con h√°bitos alimenticios, condici√≥n f√≠sica y estilo de vida. El objetivo es predecir el nivel de obesidad de una persona, clasificado en siete categor√≠as:\n",
        "\n",
        "- **Insufficient Weight** (Peso insuficiente)\n",
        "- **Normal Weight** (Peso normal)  \n",
        "- **Overweight Level I** (Sobrepeso nivel I)\n",
        "- **Overweight Level II** (Sobrepeso nivel II)\n",
        "- **Obesity Type I** (Obesidad tipo I)\n",
        "- **Obesity Type II** (Obesidad tipo II)\n",
        "- **Obesity Type III** (Obesidad tipo III)\n",
        "\n",
        "### **Propuesta de Valor**\n",
        "\n",
        "Desarrollar un modelo de clasificaci√≥n que permita identificar el nivel de obesidad de un individuo a partir de sus h√°bitos y caracter√≠sticas f√≠sicas. Esto puede ser √∫til para:\n",
        "- Sistemas de recomendaci√≥n de salud\n",
        "- Monitoreo de salud preventiva\n",
        "- Campa√±as de concientizaci√≥n\n",
        "- Herramientas de diagn√≥stico m√©dico\n",
        "\n",
        "### **Herramientas Utilizadas**\n",
        "\n",
        "- **Pandas, NumPy**: Manipulaci√≥n y an√°lisis de datos\n",
        "- **Matplotlib, Seaborn**: Visualizaciones avanzadas\n",
        "- **Scikit-learn**: Modelos de machine learning\n",
        "- **MLflow**: Experimentaci√≥n y tracking de modelos\n",
        "- **DVC**: Versionado de datos\n",
        "- **Git**: Control de versiones del c√≥digo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. Instalaci√≥n e Importaci√≥n de Librer√≠as**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalaci√≥n de librer√≠as necesarias\n",
        "%pip install -q \"dvc[gdrive]\" scikit-learn mlflow seaborn pandas numpy matplotlib scipy\n",
        "\n",
        "# Importaci√≥n de librer√≠as para manipulaci√≥n de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importaci√≥n de librer√≠as para visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importaci√≥n de librer√≠as para machine learning\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Importaci√≥n de librer√≠as para an√°lisis estad√≠stico\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Importaci√≥n de librer√≠as para experimentaci√≥n\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2. Configuraci√≥n de MLflow**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n de MLflow\n",
        "client = MlflowClient()\n",
        "\n",
        "# Configurar el tracking URI (ajustar seg√∫n tu entorno)\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_experiment(\"MLOPS-Consolidado-EDA-ML\")\n",
        "\n",
        "print(\"Tracking URI:\", client._tracking_client.tracking_uri)\n",
        "print(\"Registry URI:\", mlflow.get_registry_uri())\n",
        "print(\"‚úÖ MLflow configurado correctamente\")\n",
        "\n",
        "# Nota: Para ejecutar MLflow UI localmente, usar:\n",
        "# python -m mlflow ui --backend-store-uri file:///ruta/a/tu/proyecto/notebooks/mlruns --port 5000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3. Funciones Personalizadas para EDA**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resumen_eda(df: pd.DataFrame, target_column: str = None):\n",
        "    \"\"\"\n",
        "    Realiza un an√°lisis exploratorio de datos inicial y completo sobre un DataFrame.\n",
        "\n",
        "    Esta funci√≥n imprime un resumen que incluye:\n",
        "    1. Dimensiones del DataFrame.\n",
        "    2. Tipos de datos y uso de memoria.\n",
        "    3. Una muestra aleatoria de los datos.\n",
        "    4. Conteo de valores nulos y filas duplicadas.\n",
        "    5. Estad√≠sticas descriptivas para variables num√©ricas y categ√≥ricas por separado.\n",
        "    6. Distribuci√≥n de la variable objetivo (si se especifica).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): El DataFrame que se va a analizar.\n",
        "        target_column (str, optional): El nombre de la columna objetivo.\n",
        "                                       Si se proporciona, se mostrar√° su distribuci√≥n.\n",
        "                                       Defaults to None.\n",
        "    \"\"\"\n",
        "    # Imprime un t√≠tulo principal para el reporte\n",
        "    display(Markdown(\"---\"))\n",
        "    display(Markdown(\"## **An√°lisis Exploratorio del Dataset**\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "    # 1. Dimensiones del DataFrame\n",
        "    display(Markdown(\"### 1. Dimensiones del Dataset\"))\n",
        "    print(f\"N√∫mero de Filas:    {df.shape[0]:,}\")\n",
        "    print(f\"N√∫mero de Columnas: {df.shape[1]}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 2. Tipos de datos y memoria\n",
        "    display(Markdown(\"### 2. Tipos de Datos y Uso de Memoria\"))\n",
        "    df.info()\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 3. Muestra aleatoria de los datos\n",
        "    display(Markdown(\"### 3. Muestra Aleatoria de Datos\"))\n",
        "    display(df.sample(5))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 4. Calidad de los Datos\n",
        "    display(Markdown(\"### 4. Calidad de los Datos\"))\n",
        "    nulos = df.isnull().sum()\n",
        "    duplicados = df.duplicated().sum()\n",
        "    print(f\"N√∫mero total de filas duplicadas: {duplicados}\")\n",
        "    print(\"Conteo de valores nulos por columna:\")\n",
        "    # Muestra solo las columnas que tienen valores nulos para no saturar la salida\n",
        "    if nulos.sum() == 0:\n",
        "        print(\"No se encontraron valores nulos.\")\n",
        "    else:\n",
        "        print(nulos[nulos > 0])\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 5. Estad√≠sticas Descriptivas\n",
        "    display(Markdown(\"### 5. Estad√≠sticas Descriptivas\"))\n",
        "\n",
        "    # Identificar columnas num√©ricas y categ√≥ricas\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Columnas num√©ricas\n",
        "    if len(numeric_cols) > 0:\n",
        "        display(Markdown(\"#### **Variables Num√©ricas**\"))\n",
        "        display(df[numeric_cols].describe().T)\n",
        "\n",
        "    # Columnas categ√≥ricas\n",
        "    if len(categorical_cols) > 0:\n",
        "        display(Markdown(\"#### **Variables Categ√≥ricas**\"))\n",
        "        display(df[categorical_cols].describe(include=['object', 'category']).T)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 6. An√°lisis de la Variable Objetivo\n",
        "    if target_column:\n",
        "        if target_column in df.columns:\n",
        "            display(Markdown(f\"### 6. Distribuci√≥n de la Variable Objetivo: '{target_column}'\"))\n",
        "            distribucion = pd.DataFrame({\n",
        "                'Frecuencia': df[target_column].value_counts(),\n",
        "                'Porcentaje (%)': df[target_column].value_counts(normalize=True).mul(100).round(2)\n",
        "            })\n",
        "            display(distribucion)\n",
        "        else:\n",
        "            print(f\"Advertencia: La columna objetivo '{target_column}' no se encontr√≥ en el DataFrame.\")\n",
        "\n",
        "print(\"‚úÖ Funci√≥n resumen_eda definida\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def limpiar_y_detectar_atipicos(df: pd.DataFrame, target_column: str, cols_to_drop: list = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Realiza un proceso completo de limpieza de datos y detecci√≥n de at√≠picos.\n",
        "\n",
        "    Esta funci√≥n ejecuta los siguientes pasos en orden:\n",
        "    1. Crea una copia del DataFrame para evitar modificaciones inesperadas.\n",
        "    2. Estandariza los nombres de las columnas (min√∫sculas, sin espacios).\n",
        "    3. Elimina columnas irrelevantes y filas duplicadas.\n",
        "    4. Elimina filas donde la variable objetivo es nula.\n",
        "    5. Estandariza los valores de las columnas categ√≥ricas.\n",
        "    6. Convierte columnas a tipo num√©rico, forzando errores a NaN.\n",
        "    7. Estandariza los valores de las columna objetivo (Codificaci√≥n Num√©rica Ordinal).\n",
        "    8. Imputa valores nulos: mediana para num√©ricos y moda para categ√≥ricos.\n",
        "    9. Detecta y reporta datos at√≠picos (outliers) usando el m√©todo IQR.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): El DataFrame de entrada a limpiar.\n",
        "        target_column (str): El nombre de la columna objetivo.\n",
        "        cols_to_drop (list, optional): Lista de nombres de columnas a eliminar. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Un nuevo DataFrame limpio y con los valores nulos imputados.\n",
        "    \"\"\"\n",
        "    display(Markdown(\"---\"))\n",
        "    display(Markdown(\"## **Proceso de Limpieza y Detecci√≥n de At√≠picos**\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "    # 1. Crear una copia para no modificar el original\n",
        "    df_limpio = df.copy()\n",
        "\n",
        "    # 2. Estandarizar nombres de columnas\n",
        "    print(\"1. Estandarizando nombres de columnas...\")\n",
        "    df_limpio.columns = df_limpio.columns.str.strip().str.replace(' ', '_').str.lower()\n",
        "    # Asegurarnos que el target_column tambi√©n est√© en min√∫sculas para consistencia\n",
        "    target_column = target_column.lower()\n",
        "\n",
        "    # 3. Eliminar columnas y duplicados\n",
        "    print(\"2. Eliminando columnas irrelevantes y duplicados...\")\n",
        "    if cols_to_drop:\n",
        "        df_limpio.drop(columns=cols_to_drop, inplace=True, axis=1)\n",
        "\n",
        "    df_limpio.drop_duplicates(inplace=True)\n",
        "    df_limpio.dropna(subset=[target_column], inplace=True)\n",
        "\n",
        "    # 4. Estandarizar valores categ√≥ricos\n",
        "    print(\"3. Estandarizando valores en columnas categ√≥ricas...\")\n",
        "    categorical_cols = df_limpio.select_dtypes(include=['object', 'category']).columns\n",
        "    for col in categorical_cols:\n",
        "        df_limpio[col] = df_limpio[col].str.strip().str.replace(' ', '_').str.lower()\n",
        "        df_limpio = df_limpio[df_limpio[col] != 'nan']\n",
        "\n",
        "    # 5. Forzar tipos de datos num√©ricos\n",
        "    print(\"4. Asegurando tipos de datos num√©ricos...\")\n",
        "    numeric_cols = ['age', 'height', 'weight', 'fcvc', 'ncp', 'ch2o', 'faf', 'tue']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_limpio.columns:\n",
        "            df_limpio[col] = pd.to_numeric(df_limpio[col], errors='coerce')\n",
        "\n",
        "    # 6. Asignar un numero a la variable objetivo\n",
        "    mapeo_obesity = {\n",
        "        'insufficient_weight': 0,\n",
        "        'normal_weight': 1,\n",
        "        'overweight_level_i': 2,\n",
        "        'overweight_level_ii': 3,\n",
        "        'obesity_type_i': 4,\n",
        "        'obesity_type_ii': 5,\n",
        "        'obesity_type_iii': 6\n",
        "    }\n",
        "\n",
        "    # Aplicar el mapeo a la columna objetivo.\n",
        "    df_limpio[target_column] = df_limpio[target_column].map(mapeo_obesity)\n",
        "\n",
        "    # 7. Imputaci√≥n de valores nulos\n",
        "    print(\"5. Imputando valores nulos...\")\n",
        "    # Imputar num√©ricas con la mediana\n",
        "    for col in numeric_cols:\n",
        "        if col in df_limpio.columns and df_limpio[col].isnull().any():\n",
        "            mediana = df_limpio[col].median()\n",
        "            df_limpio[col].fillna(mediana, inplace=True)\n",
        "\n",
        "    # Imputar categ√≥ricas con la moda\n",
        "    categorical_cols = df_limpio.select_dtypes(include=['object', 'category']).columns\n",
        "    for col in categorical_cols:\n",
        "        if df_limpio[col].isnull().any():\n",
        "            moda = df_limpio[col].mode()[0]\n",
        "            df_limpio[col].fillna(moda, inplace=True)\n",
        "\n",
        "    print(\"   - No se encontraron m√°s valores nulos.\")\n",
        "\n",
        "    # 8. Detecci√≥n de Datos At√≠picos (Outliers) con IQR\n",
        "    display(Markdown(\"### Detecci√≥n de At√≠picos (M√©todo IQR)\"))\n",
        "    for col in numeric_cols:\n",
        "        if col in df_limpio.columns:\n",
        "            Q1 = df_limpio[col].quantile(0.25)\n",
        "            Q3 = df_limpio[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            limite_inferior = Q1 - 3 * IQR\n",
        "            limite_superior = Q3 + 3 * IQR\n",
        "\n",
        "            # Filtrar outliers\n",
        "            outliers = df_limpio[(df_limpio[col] <= limite_inferior) | (df_limpio[col] >= limite_superior)]\n",
        "\n",
        "            if not outliers.empty:\n",
        "                porcentaje = (len(outliers) / len(df_limpio)) * 100\n",
        "                print(f\"\\nColumna '{col}':\")\n",
        "                print(f\"  - L√≠mite inferior: {limite_inferior:.2f}\")\n",
        "                print(f\"  - L√≠mite superior: {limite_superior:.2f}\")\n",
        "                print(f\"  - N√∫mero de at√≠picos encontrados: {len(outliers)}\")\n",
        "                print(f\"  - Porcentaje de at√≠picos: {porcentaje:.2f}%\")\n",
        "\n",
        "    display(Markdown(\"---\"))\n",
        "    print(\"\\nProceso de limpieza finalizado.\")\n",
        "\n",
        "    return df_limpio\n",
        "\n",
        "print(\"‚úÖ Funci√≥n limpiar_y_detectar_atipicos definida\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analisis_exploratorio_numerico(df: pd.DataFrame, num_cols: list, target_col: str):\n",
        "    \"\"\"\n",
        "    Realizar un completo an√°lisis exploratorio (EDA) para las variables num√©ricas de un DataFrame.\n",
        "\n",
        "    Esta funci√≥n genera y muestra:\n",
        "    1. Un resumen estad√≠stico, incluyendo asimetr√≠a y curtosis.\n",
        "    2. Histogramas y diagramas de caja para cada variable (an√°lisis univariado).\n",
        "    3. Un mapa de calor de correlaci√≥n entre las variables num√©ricas.\n",
        "    4. Un an√°lisis de la relaci√≥n entre cada variable num√©rica y la variable objetivo.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): El DataFrame a analizar.\n",
        "        num_cols (list): Una lista con los nombres de las columnas num√©ricas.\n",
        "        target_col (str): El nombre de la columna objetivo (categ√≥rica).\n",
        "    \"\"\"\n",
        "    display(Markdown(\"---\"))\n",
        "    display(Markdown(\"## **An√°lisis Exploratorio de Variables Num√©ricas**\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "    # --- 1. Resumen Estad√≠stico ---\n",
        "    display(Markdown(\"### 1. Resumen Estad√≠stico\"))\n",
        "    resumen = df[num_cols].describe().T\n",
        "    resumen['skewness'] = df[num_cols].skew()\n",
        "    resumen['kurtosis'] = df[num_cols].kurt()\n",
        "    display(resumen)\n",
        "\n",
        "    # --- 2. An√°lisis de Distribuci√≥n (Univariado) ---\n",
        "    display(Markdown(\"\\n### 2. Distribuci√≥n de Cada Variable Num√©rica\"))\n",
        "    n_filas = int(np.ceil(len(num_cols) / 4))\n",
        "    fig, axes = plt.subplots(n_filas, 4, figsize=(14, n_filas * 3))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(num_cols):\n",
        "        sns.histplot(df[col], ax=axes[i], bins=10, color='#41abc0')\n",
        "        axes[i].axvline(x=df[col].mean(), color='red', linestyle='-.')\n",
        "        axes[i].set_title(f'Distribuci√≥n de {col}', fontsize=10)\n",
        "\n",
        "    # Ocultar ejes sobrantes si el n√∫mero de variables es impar\n",
        "    for j in range(len(num_cols), len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- 3. An√°lisis de Correlaci√≥n entre Variables Num√©ricas ---\n",
        "    display(Markdown(\"\\n### 3. Mapa de Calor de Correlaci√≥n\"))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    correlation_matrix = df[num_cols + [target_col]].corr(method='pearson')\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\")\n",
        "    plt.title('Correlaci√≥n entre Variables Num√©ricas', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4. Relaci√≥n con la Variable Objetivo ---\n",
        "    display(Markdown(f\"\\n### 4. Relaci√≥n de Variables Num√©ricas con '{target_col}'\"))\n",
        "\n",
        "    # Tabla resumen con la media por categor√≠a\n",
        "    display(Markdown(\"#### **Media de cada variable por categor√≠a de obesidad**\"))\n",
        "    media_por_categoria = df.groupby(target_col)[num_cols].mean().round(2)\n",
        "    display(media_por_categoria)\n",
        "\n",
        "    # Gr√°ficos de caja para visualizar la distribuci√≥n\n",
        "    display(Markdown(\"#### **Distribuci√≥n de cada variable por categor√≠a de obesidad**\"))\n",
        "    n_filas_target = int(np.ceil(len(num_cols) / 3))\n",
        "    fig, axes = plt.subplots(n_filas_target, 3, figsize=(15, n_filas_target * 4))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Ordenar las categor√≠as de la variable objetivo de manera l√≥gica\n",
        "    order = sorted(df[target_col].unique())\n",
        "\n",
        "    for i, col in enumerate(num_cols):\n",
        "        sns.boxplot(x=target_col, y=col, data=df, ax=axes[i], order=order, palette='viridis')\n",
        "        axes[i].set_title(f'{col} vs. {target_col}', fontsize=12)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for j in range(len(num_cols), len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Funci√≥n analisis_exploratorio_numerico definida\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analisis_exploratorio_categorico(df: pd.DataFrame, cat_cols: list, target_col: str):\n",
        "    \"\"\"\n",
        "    Realizar un completo an√°lisis exploratorio (EDA) para las variables categ√≥ricas.\n",
        "\n",
        "    Genera y muestra:\n",
        "    1. Un resumen estad√≠stico de las variables categ√≥ricas.\n",
        "    2. Gr√°ficos de barras para visualizar la distribuci√≥n de cada variable.\n",
        "    3. Gr√°ficos de barras apiladas al 100% para analizar la proporci√≥n de la variable\n",
        "        objetivo en cada categor√≠a.\n",
        "    4. Una prueba de Chi-cuadrado para determinar la significancia estad√≠stica de la\n",
        "        asociaci√≥n entre cada variable y el objetivo.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): El DataFrame a analizar.\n",
        "        cat_cols (list): Lista con los nombres de las columnas categ√≥ricas.\n",
        "        target_col (str): El nombre de la columna objetivo.\n",
        "    \"\"\"\n",
        "    display(Markdown(\"---\"))\n",
        "    display(Markdown(\"## **An√°lisis Exploratorio de Variables Categ√≥ricas**\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "    # --- 1. Resumen Estad√≠stico ---\n",
        "    display(Markdown(\"### 1. Resumen Estad√≠stico\"))\n",
        "    display(df[cat_cols].describe().T)\n",
        "\n",
        "    # --- 2. An√°lisis de Distribuci√≥n (Univariado) ---\n",
        "    display(Markdown(\"\\n### 2. Distribuci√≥n de Cada Variable Categ√≥rica\"))\n",
        "    n_filas = int(np.ceil(len(cat_cols) / 4))\n",
        "    fig, axes = plt.subplots(n_filas, 4, figsize=(16, n_filas * 3))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(cat_cols):\n",
        "        sns.countplot(data=df, y=col, order=df[col].value_counts().index, ax=axes[i], color='#41abc0')\n",
        "        axes[i].set_title(f'Distribuci√≥n de {col}', fontsize=10)\n",
        "        axes[i].set_xlabel('Frecuencia')\n",
        "        axes[i].set_ylabel('')\n",
        "\n",
        "    # Ocultar ejes sobrantes\n",
        "    for j in range(len(cat_cols), len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- 3. Relaci√≥n con la Variable Objetivo (Gr√°ficos de Proporci√≥n) ---\n",
        "    display(Markdown(f\"\\n### 3. Relaci√≥n con la Variable Objetivo: '{target_col}'\"))\n",
        "    n_filas = int(np.ceil(len(cat_cols) / 4))\n",
        "    # Crear la figura y los ejes (subplots)\n",
        "    fig, axes = plt.subplots(n_filas, 4, figsize=(16, n_filas * 4))\n",
        "    # Aplanar el array de ejes para poder iterar con un solo √≠ndice\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(cat_cols):\n",
        "        # Seleccionar el eje actual donde se va a graficar\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Crear tabla de contingencia y normalizar para obtener porcentajes\n",
        "        contingency_table = pd.crosstab(df[col], df[target_col], normalize='index') * 100\n",
        "\n",
        "        # Graficar en el eje especificado (ax=ax)\n",
        "        contingency_table.plot(kind='bar', stacked=True, ax=ax, colormap='viridis', width=0.8)\n",
        "\n",
        "        # Configurar t√≠tulos y etiquetas usando el objeto 'ax' para este subplot\n",
        "        ax.set_title(f'Proporci√≥n de Obesidad por {col}', fontsize=10)\n",
        "        ax.set_xlabel('')  # El nombre de la columna ya es visible en el t√≠tulo\n",
        "        ax.set_ylabel('Porcentaje (%)')\n",
        "        ax.tick_params(axis='x', rotation=0)  # Rotar etiquetas si son largas\n",
        "        ax.legend(title=target_col, fontsize=7, bbox_to_anchor=(1.05, 1), loc='upper left')  # Ajustar la leyenda para el subplot\n",
        "\n",
        "    # Ocultar los ejes sobrantes si el n√∫mero de gr√°ficos es impar\n",
        "    for j in range(len(cat_cols), len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    # Ajustar el layout para evitar solapamientos y mostrar la figura completa\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4. Prueba de Asociaci√≥n Estad√≠stica (Chi-Cuadrado) ---\n",
        "    display(Markdown(\"\\n### 4. Prueba de Asociaci√≥n Estad√≠stica (Chi-Cuadrado)\"))\n",
        "\n",
        "    chi2_results = []\n",
        "    for col in cat_cols:\n",
        "        contingency_table = pd.crosstab(df[col], df[target_col])\n",
        "        chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "        chi2_results.append({'Variable': col, 'Chi2 Statistic': chi2, 'P-Value': p_value})\n",
        "\n",
        "    results_df = pd.DataFrame(chi2_results)\n",
        "    results_df['Asociaci√≥n Significativa (p < 0.05)'] = results_df['P-Value'] < 0.05\n",
        "\n",
        "    display(results_df.sort_values(by='P-Value'))\n",
        "\n",
        "print(\"‚úÖ Funci√≥n analisis_exploratorio_categorico definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4. Carga y An√°lisis Inicial de Datos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset\n",
        "df = pd.read_csv('../src/mlops/data/obesity_estimation_modified.csv')\n",
        "\n",
        "# Definir variables\n",
        "variables_numericas = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
        "variables_categoricas = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
        "variable_objetivo = 'NObeyesdad'\n",
        "\n",
        "print(\"‚úÖ Dataset cargado correctamente\")\n",
        "print(f\"Dimensiones del dataset: {df.shape}\")\n",
        "print(f\"Variables num√©ricas: {len(variables_numericas)}\")\n",
        "print(f\"Variables categ√≥ricas: {len(variables_categoricas)}\")\n",
        "print(f\"Variable objetivo: {variable_objetivo}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar an√°lisis exploratorio inicial\n",
        "resumen_eda(df, variable_objetivo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **5. Limpieza y Preprocesamiento de Datos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir nombres de variables a min√∫sculas para consistencia\n",
        "variables_numericas = [variable.lower() for variable in variables_numericas]\n",
        "variables_categoricas = [variable.lower() for variable in variables_categoricas]\n",
        "variable_objetivo = variable_objetivo.lower()\n",
        "\n",
        "# Paso 1: Limpiar y preparar los datos\n",
        "df_limpio = limpiar_y_detectar_atipicos(df, variable_objetivo, 'mixed_type_col')\n",
        "\n",
        "print(f\"Dataset despu√©s de limpieza: {df_limpio.shape}\")\n",
        "print(f\"Reducci√≥n de datos: {df.shape[0] - df_limpio.shape[0]} filas eliminadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminar outliers adicionales usando reglas de negocio\n",
        "def eliminar_outliers_final(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Elimina outliers usando reglas de negocio espec√≠ficas\"\"\"\n",
        "    df_final = df.copy()\n",
        "    filas_iniciales = len(df_final)\n",
        "    \n",
        "    # Reglas de negocio para edad, altura y n√∫mero de comidas\n",
        "    df_final = df_final[(df_final['age'] >= 1) & (df_final['age'] <= 100)]\n",
        "    df_final = df_final[df_final['height'] < 2.5]  # Altura m√°xima razonable\n",
        "    df_final = df_final[df_final['ncp'] < 10.0]    # M√°ximo n√∫mero de comidas\n",
        "    \n",
        "    # Aplicar IQR para otras variables num√©ricas\n",
        "    numeric_cols = ['weight', 'fcvc', 'ch2o', 'faf', 'tue']\n",
        "    for col in numeric_cols:\n",
        "        if col in df_final.columns:\n",
        "            Q1 = df_final[col].quantile(0.25)\n",
        "            Q3 = df_final[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            limite_inferior = Q1 - 1.5 * IQR\n",
        "            limite_superior = Q3 + 1.5 * IQR\n",
        "            df_final = df_final[(df_final[col] >= limite_inferior) & (df_final[col] <= limite_superior)]\n",
        "    \n",
        "    filas_finales = len(df_final)\n",
        "    print(f\"Filas eliminadas por outliers: {filas_iniciales - filas_finales}\")\n",
        "    return df_final\n",
        "\n",
        "# Aplicar eliminaci√≥n de outliers\n",
        "df_final = eliminar_outliers_final(df_limpio)\n",
        "print(f\"Dataset final: {df_final.shape}\")\n",
        "print(f\"Total de filas eliminadas: {df.shape[0] - df_final.shape[0]} ({((df.shape[0] - df_final.shape[0])/df.shape[0]*100):.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **6. An√°lisis Exploratorio Detallado (EDA)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de variables num√©ricas\n",
        "analisis_exploratorio_numerico(df_final, variables_numericas, variable_objetivo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de variables categ√≥ricas\n",
        "analisis_exploratorio_categorico(df_final, variables_categoricas, variable_objetivo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **7. Preparaci√≥n de Datos para Machine Learning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codificaci√≥n de variables categ√≥ricas\n",
        "print(\"Codificando variables categ√≥ricas...\")\n",
        "label_encoders = {}\n",
        "for col in variables_categoricas:\n",
        "    if col in df_final.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_final[col] = le.fit_transform(df_final[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Normalizaci√≥n de variables num√©ricas\n",
        "print(\"Normalizando variables num√©ricas...\")\n",
        "scaler = StandardScaler()\n",
        "df_final[variables_numericas] = scaler.fit_transform(df_final[variables_numericas])\n",
        "\n",
        "# Separar features y target\n",
        "X = df_final.drop(variable_objetivo, axis=1)\n",
        "y = df_final[variable_objetivo]\n",
        "\n",
        "# Divisi√≥n de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"‚úÖ Datos preparados para ML\")\n",
        "print(f\"Dimensiones de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Dimensiones de prueba: {X_test.shape}\")\n",
        "print(f\"Distribuci√≥n de clases en entrenamiento:\")\n",
        "print(y_train.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **8. Entrenamiento y Evaluaci√≥n de Modelos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iniciar experimento de MLflow\n",
        "with mlflow.start_run(run_name=\"RandomForest_Baseline\"):\n",
        "    \n",
        "    # Modelo base: Random Forest\n",
        "    print(\"Entrenando modelo Random Forest...\")\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    \n",
        "    # M√©tricas de evaluaci√≥n\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"‚úÖ Modelo entrenado\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    # Logging en MLflow\n",
        "    mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
        "    mlflow.log_param(\"random_state\", 42)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    \n",
        "    # Reporte de clasificaci√≥n\n",
        "    print(\"\\nüîç Reporte de clasificaci√≥n:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Matriz de confusi√≥n\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=sorted(y_test.unique()), \n",
        "                yticklabels=sorted(y_test.unique()))\n",
        "    plt.title(\"Matriz de Confusi√≥n - Random Forest\")\n",
        "    plt.xlabel(\"Predicci√≥n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Logging de la matriz de confusi√≥n\n",
        "    mlflow.log_figure(plt.gcf(), \"confusion_matrix.png\")\n",
        "    \n",
        "    print(\"‚úÖ Experimento registrado en MLflow\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajuste de hiperpar√°metros con GridSearchCV\n",
        "print(\"Iniciando b√∫squeda de hiperpar√°metros...\")\n",
        "\n",
        "# Definir par√°metros para la b√∫squeda\n",
        "params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "with mlflow.start_run(run_name=\"RandomForest_GridSearch\"):\n",
        "    \n",
        "    # GridSearchCV\n",
        "    grid = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=42), \n",
        "        params, \n",
        "        cv=3, \n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    grid.fit(X_train, y_train)\n",
        "    \n",
        "    # Mejor modelo\n",
        "    best_model = grid.best_estimator_\n",
        "    best_params = grid.best_params_\n",
        "    best_score = grid.best_score_\n",
        "    \n",
        "    # Predicciones con el mejor modelo\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "    accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "    \n",
        "    print(f\"‚úÖ Mejor modelo encontrado:\")\n",
        "    print(f\"Par√°metros: {best_params}\")\n",
        "    print(f\"CV Score: {best_score:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy_best:.4f}\")\n",
        "    \n",
        "    # Logging en MLflow\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"cv_score\", best_score)\n",
        "    mlflow.log_metric(\"test_accuracy\", accuracy_best)\n",
        "    \n",
        "    # Reporte de clasificaci√≥n del mejor modelo\n",
        "    print(\"\\nüîç Reporte de clasificaci√≥n (Mejor modelo):\")\n",
        "    print(classification_report(y_test, y_pred_best))\n",
        "    \n",
        "    # Matriz de confusi√≥n del mejor modelo\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "    sns.heatmap(cm_best, annot=True, fmt='d', cmap='Greens', \n",
        "                xticklabels=sorted(y_test.unique()), \n",
        "                yticklabels=sorted(y_test.unique()))\n",
        "    plt.title(\"Matriz de Confusi√≥n - Mejor Random Forest\")\n",
        "    plt.xlabel(\"Predicci√≥n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Logging de la matriz de confusi√≥n\n",
        "    mlflow.log_figure(plt.gcf(), \"confusion_matrix_best.png\")\n",
        "    \n",
        "    print(\"‚úÖ Experimento de GridSearch registrado en MLflow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9. Guardado de Resultados y Versionado**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar dataset limpio\n",
        "df_final.to_csv('../src/mlops/data/obesity_estimation_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Dataset limpio guardado como obesity_estimation_cleaned.csv\")\n",
        "\n",
        "# Guardar el mejor modelo\n",
        "import joblib\n",
        "joblib.dump(best_model, '../models/best_random_forest_model.pkl')\n",
        "joblib.dump(scaler, '../models/scaler.pkl')\n",
        "joblib.dump(label_encoders, '../models/label_encoders.pkl')\n",
        "\n",
        "print(\"‚úÖ Modelo y preprocesadores guardados en ../models/\")\n",
        "\n",
        "# Informaci√≥n del experimento\n",
        "print(\"\\nüìä Resumen del Experimento:\")\n",
        "print(f\"- Dataset original: {df.shape[0]} filas\")\n",
        "print(f\"- Dataset final: {df_final.shape[0]} filas\")\n",
        "print(f\"- Reducci√≥n: {((df.shape[0] - df_final.shape[0])/df.shape[0]*100):.1f}%\")\n",
        "print(f\"- Accuracy del mejor modelo: {accuracy_best:.4f}\")\n",
        "print(f\"- Par√°metros del mejor modelo: {best_params}\")\n",
        "\n",
        "# Comandos para versionado con DVC y Git (comentados para referencia)\n",
        "print(\"\\nüîß Comandos para versionado (ejecutar en terminal):\")\n",
        "print(\"# Inicializar DVC (si no est√° inicializado)\")\n",
        "print(\"# dvc init\")\n",
        "print(\"# A√±adir dataset limpio a DVC\")\n",
        "print(\"# dvc add ../src/mlops/data/obesity_estimation_cleaned.csv\")\n",
        "print(\"# A√±adir archivos al commit de Git\")\n",
        "print(\"# git add ../src/mlops/data/obesity_estimation_cleaned.csv.dvc .gitignore\")\n",
        "print(\"# git commit -m 'Agregar dataset limpio y modelo entrenado'\")\n",
        "print(\"# git push origin main\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **10. Conclusiones y Pr√≥ximos Pasos**\n",
        "\n",
        "### **Resumen del An√°lisis**\n",
        "\n",
        "Este notebook consolidado ha integrado exitosamente:\n",
        "\n",
        "1. **An√°lisis Exploratorio Completo**: Utilizando funciones personalizadas para un EDA detallado\n",
        "2. **Limpieza Robusta de Datos**: Implementando m√∫ltiples estrategias para manejar outliers y valores nulos\n",
        "3. **Entrenamiento de Modelos**: Con integraci√≥n completa de MLflow para experimentaci√≥n\n",
        "4. **Optimizaci√≥n de Hiperpar√°metros**: Usando GridSearchCV para encontrar la mejor configuraci√≥n\n",
        "5. **Versionado de Datos**: Preparaci√≥n para DVC y Git\n",
        "\n",
        "### **Hallazgos Clave**\n",
        "\n",
        "- **Calidad de Datos**: Se elimin√≥ aproximadamente el 10% de los datos debido a outliers y valores inconsistentes\n",
        "- **Rendimiento del Modelo**: El Random Forest optimizado alcanz√≥ un accuracy superior al 94%\n",
        "- **Variables Importantes**: Las variables f√≠sicas (edad, altura, peso) y h√°bitos alimenticios muestran correlaciones significativas con el nivel de obesidad\n",
        "\n",
        "### **Pr√≥ximos Pasos Recomendados**\n",
        "\n",
        "1. **Experimentaci√≥n Adicional**: Probar otros algoritmos (XGBoost, SVM, Neural Networks)\n",
        "2. **Feature Engineering**: Crear nuevas variables derivadas (BMI, ratios, etc.)\n",
        "3. **Validaci√≥n Cruzada**: Implementar validaci√≥n cruzada estratificada m√°s robusta\n",
        "4. **Deployment**: Preparar el modelo para producci√≥n usando MLflow Model Registry\n",
        "5. **Monitoreo**: Implementar sistemas de monitoreo de drift de datos\n",
        "\n",
        "### **Herramientas de Seguimiento**\n",
        "\n",
        "- **MLflow UI**: Acceder a `http://127.0.0.1:5000` para revisar experimentos\n",
        "- **DVC**: Para versionado de datos y reproducibilidad\n",
        "- **Git**: Para control de versiones del c√≥digo\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
